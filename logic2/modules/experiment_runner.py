"""ì‹¤í—˜ ì‹¤í–‰ ê´€ë¦¬ ëª¨ë“ˆ"""

import pandas as pd
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import traceback
import json
import os

from .data_loader import DataLoader
from .experiment_config import ExperimentConfig
from .file_manager import FileManager
from .optimizer import SKUOptimizer
from .analyzer import ResultAnalyzer
from .visualizer import ResultVisualizer


class ExperimentRunner:
    """ì‹¤í—˜ ì‹¤í–‰ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config_file: str = None):
        """
        ì‹¤í—˜ ì‹¤í–‰ê¸° ì´ˆê¸°í™”
        
        Args:
            config_file: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.logger = logging.getLogger(__name__)
        self.config = ExperimentConfig(config_file)
        self.results = {}  # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
        
    def run_single_experiment(self, sku_file: str, store_file: str, 
                            scenario_name: str, output_dir: str = None) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            sku_file: SKU ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            store_file: ë§¤ì¥ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            scenario_name: ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict[str, Any]: ì‹¤í—˜ ê²°ê³¼
        """
        self.logger.info(f"ë‹¨ì¼ ì‹¤í—˜ ì‹œì‘: {scenario_name}")
        start_time = time.time()
        
        try:
            # 1. ë°ì´í„° ë¡œë”©
            data_loader = DataLoader()
            data = data_loader.load_and_preprocess(sku_file, store_file)
            
            # 2. ì‹œë‚˜ë¦¬ì˜¤ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            params = self.config.get_scenario_params(scenario_name)
            if not params:
                raise ValueError(f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # 3. íŒŒì¼ ê´€ë¦¬ì ì„¤ì •
            file_manager = FileManager(base_output_dir=output_dir or "../output")
            experiment_dir = file_manager.create_experiment_folder(scenario_name)
            
            # 4. ìµœì í™” ì‹¤í–‰
            optimizer = SKUOptimizer(data, params)
            optimization_result = optimizer.optimize()
            
            if not optimization_result['success']:
                raise RuntimeError(f"ìµœì í™” ì‹¤íŒ¨: {optimization_result['message']}")
            
            # 5. ê²°ê³¼ ë¶„ì„
            analyzer = ResultAnalyzer(data, optimizer)
            analysis_results = analyzer.analyze()
            
            # 6. ì‹œê°í™” ìƒì„±
            visualizer = ResultVisualizer(experiment_dir)
            plot_files = visualizer.create_plots(analysis_results, scenario_name)
            
            # 7. ê²°ê³¼ ì €ì¥
            allocation_df = optimizer.get_allocation_results()
            store_summary_df = optimizer.get_store_summary()
            
            file_manager.save_experiment_results(
                experiment_dir, 
                scenario_name,
                allocation_df,
                store_summary_df,
                analysis_results,
                params,
                optimization_result
            )
            
            # 8. ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = time.time() - start_time
            
            # 9. ê²°ê³¼ íŒ¨í‚¤ì§•
            experiment_result = {
                'scenario_name': scenario_name,
                'success': True,
                'execution_time': execution_time,
                'experiment_dir': experiment_dir,
                'optimization_result': optimization_result,
                'analysis_results': analysis_results,
                'plot_files': plot_files,
                'summary_stats': analyzer.get_summary_stats(),
                'params': params,
                'timestamp': datetime.now().isoformat()
            }
            
            self.results[scenario_name] = experiment_result
            
            self.logger.info(f"ì‹¤í—˜ ì™„ë£Œ: {scenario_name} (ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ)")
            return experiment_result
            
        except Exception as e:
            error_msg = f"ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.logger.error(error_msg)
            self.logger.error(traceback.format_exc())
            
            return {
                'scenario_name': scenario_name,
                'success': False,
                'error': error_msg,
                'execution_time': time.time() - start_time,
                'timestamp': datetime.now().isoformat()
            }
    
    def run_batch_experiments(self, sku_file: str, store_file: str, 
                            scenario_names: List[str] = None, 
                            output_dir: str = None) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            sku_file: SKU ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            store_file: ë§¤ì¥ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            scenario_names: ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict[str, Any]: ì „ì²´ ë°°ì¹˜ ì‹¤í—˜ ê²°ê³¼
        """
        self.logger.info("ë°°ì¹˜ ì‹¤í—˜ ì‹œì‘")
        batch_start_time = time.time()
        
        # ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ ê²°ì •
        if scenario_names is None:
            scenario_names = list(self.config.scenarios.keys())
        
        self.logger.info(f"ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤: {scenario_names}")
        
        batch_results = {}
        successful_experiments = 0
        failed_experiments = 0
        
        for scenario_name in scenario_names:
            self.logger.info(f"ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰: {scenario_name}")
            
            result = self.run_single_experiment(sku_file, store_file, scenario_name, output_dir)
            batch_results[scenario_name] = result
            
            if result['success']:
                successful_experiments += 1
            else:
                failed_experiments += 1
        
        # ë°°ì¹˜ ì‹¤í—˜ ê²°ê³¼ ìƒì„±
        total_time = time.time() - batch_start_time
        
        batch_summary = {
            'total_experiments': len(scenario_names),
            'successful_experiments': successful_experiments,
            'failed_experiments': failed_experiments,
            'total_execution_time': total_time,
            'average_execution_time': total_time / len(scenario_names),
            'scenario_results': batch_results,
            'timestamp': datetime.now().isoformat()
        }
        
        # ë°°ì¹˜ ë¹„êµ ë¶„ì„ ìƒì„±
        if successful_experiments > 1:
            comparison_result = self.compare_experiments(
                [scenario for scenario in scenario_names 
                 if batch_results[scenario]['success']]
            )
            batch_summary['comparison_analysis'] = comparison_result
        
        self.logger.info(f"ë°°ì¹˜ ì‹¤í—˜ ì™„ë£Œ: {successful_experiments}ê°œ ì„±ê³µ, {failed_experiments}ê°œ ì‹¤íŒ¨")
        return batch_summary
    
    def run_sensitivity_analysis(self, sku_file: str, store_file: str,
                               base_scenario: str = "baseline",
                               param_variations: Dict[str, List[float]] = None,
                               output_dir: str = None) -> Dict[str, Any]:
        """
        ë¯¼ê°ë„ ë¶„ì„ ì‹¤í–‰
        
        Args:
            sku_file: SKU ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            store_file: ë§¤ì¥ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            base_scenario: ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤
            param_variations: íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ ë”•ì…”ë„ˆë¦¬
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict[str, Any]: ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
        """
        self.logger.info(f"ë¯¼ê°ë„ ë¶„ì„ ì‹œì‘: ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ {base_scenario}")
        
        if param_variations is None:
            param_variations = {
                'balance_penalty': [0.1, 0.5, 1.0, 2.0, 5.0],
                'color_coverage_weight': [0.5, 1.0, 2.0, 3.0],
                'size_coverage_weight': [0.5, 1.0, 2.0, 3.0]
            }
        
        # ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
        base_result = self.run_single_experiment(sku_file, store_file, base_scenario, output_dir)
        if not base_result['success']:
            return {
                'success': False,
                'error': f"ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ ì‹¤íŒ¨: {base_result.get('error', 'Unknown error')}"
            }
        
        sensitivity_results = {
            'base_scenario': base_scenario,
            'base_result': base_result,
            'parameter_variations': {}
        }
        
        # ê° íŒŒë¼ë¯¸í„°ë³„ ë¯¼ê°ë„ ë¶„ì„
        for param_name, values in param_variations.items():
            self.logger.info(f"íŒŒë¼ë¯¸í„° {param_name} ë¯¼ê°ë„ ë¶„ì„")
            param_results = []
            
            for value in values:
                # íŒŒë¼ë¯¸í„° ë³€ê²½ëœ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
                modified_scenario = f"{base_scenario}_sens_{param_name}_{value}"
                modified_params = base_result['params'].copy()
                modified_params[param_name] = value
                
                # ì„ì‹œ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ê°€
                self.config.scenarios[modified_scenario] = modified_params
                
                # ì‹¤í—˜ ì‹¤í–‰
                result = self.run_single_experiment(sku_file, store_file, modified_scenario, output_dir)
                result['parameter_value'] = value
                param_results.append(result)
                
                # ì„ì‹œ ì‹œë‚˜ë¦¬ì˜¤ ì œê±°
                del self.config.scenarios[modified_scenario]
            
            sensitivity_results['parameter_variations'][param_name] = param_results
        
        # ë¯¼ê°ë„ ì§€í‘œ ê³„ì‚°
        sensitivity_metrics = self._calculate_sensitivity_metrics(sensitivity_results)
        sensitivity_results['sensitivity_metrics'] = sensitivity_metrics
        
        self.logger.info("ë¯¼ê°ë„ ë¶„ì„ ì™„ë£Œ")
        return sensitivity_results
    
    def compare_experiments(self, scenario_names: List[str]) -> pd.DataFrame:
        """
        ì‹¤í—˜ ê²°ê³¼ ë¹„êµ
        
        Args:
            scenario_names: ë¹„êµí•  ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            pd.DataFrame: ë¹„êµ ê²°ê³¼ DataFrame
        """
        self.logger.info(f"ì‹¤í—˜ ë¹„êµ ë¶„ì„: {scenario_names}")
        
        comparison_data = []
        
        for scenario_name in scenario_names:
            if scenario_name not in self.results:
                self.logger.warning(f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario_name}' ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            result = self.results[scenario_name]
            if not result['success']:
                continue
            
            # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
            summary_stats = result.get('summary_stats', {})
            analysis_results = result.get('analysis_results', {})
            
            # ìŠ¤íƒ€ì¼ ë¶„ì„ ë©”íŠ¸ë¦­
            style_metrics = {}
            if 'style_analysis' in analysis_results and not analysis_results['style_analysis'].empty:
                style_df = analysis_results['style_analysis']
                style_metrics = {
                    'avg_utilization_rate': style_df['UTILIZATION_RATE'].mean(),
                    'avg_color_coverage': style_df['COLOR_COVERAGE_RATE'].mean(),
                    'avg_size_coverage': style_df['SIZE_COVERAGE_RATE'].mean(),
                    'total_styles': len(style_df)
                }
            
            # ë§¤ì¥ ì„±ê³¼ ë©”íŠ¸ë¦­
            store_metrics = {}
            if 'top_performers' in analysis_results and not analysis_results['top_performers'].empty:
                performers_df = analysis_results['top_performers']
                store_metrics = {
                    'avg_performance_score': performers_df['PERFORMANCE_SCORE'].mean(),
                    'best_performance_score': performers_df['PERFORMANCE_SCORE'].max(),
                    'avg_color_coverage': performers_df['COLOR_COVERAGE'].mean(),
                    'avg_size_coverage': performers_df['SIZE_COVERAGE'].mean()
                }
            
            # í¬ì†Œ SKU ë©”íŠ¸ë¦­
            scarce_metrics = {}
            if 'scarce_effectiveness' in analysis_results and not analysis_results['scarce_effectiveness'].empty:
                scarce_df = analysis_results['scarce_effectiveness']
                scarce_metrics = {
                    'avg_effectiveness_score': scarce_df['EFFECTIVENESS_SCORE'].mean(),
                    'best_effectiveness_score': scarce_df['EFFECTIVENESS_SCORE'].max(),
                    'total_scarce_skus': len(scarce_df)
                }
            
            # ì¢…í•© ë°ì´í„°
            comparison_row = {
                'scenario_name': scenario_name,
                'execution_time': result['execution_time'],
                'total_allocations': summary_stats.get('total_allocations', 0),
                'total_quantity': summary_stats.get('total_quantity', 0),
                'stores_covered': summary_stats.get('stores_covered', 0),
                'allocation_efficiency': summary_stats.get('allocation_efficiency', 0),
                **style_metrics,
                **store_metrics,
                **scarce_metrics
            }
            
            comparison_data.append(comparison_row)
        
        comparison_df = pd.DataFrame(comparison_data)
        
        if not comparison_df.empty:
            # ìˆœìœ„ ê³„ì‚° (ì£¼ìš” ì§€í‘œë³„)
            ranking_columns = [
                'avg_performance_score', 'allocation_efficiency', 
                'avg_utilization_rate', 'avg_effectiveness_score'
            ]
            
            for column in ranking_columns:
                if column in comparison_df.columns:
                    comparison_df[f'{column}_rank'] = comparison_df[column].rank(ascending=False)
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            if all(col in comparison_df.columns for col in ranking_columns):
                comparison_df['overall_score'] = (
                    comparison_df['avg_performance_score'] * 0.3 +
                    comparison_df['allocation_efficiency'] * 0.3 +
                    comparison_df['avg_utilization_rate'] * 0.2 +
                    comparison_df['avg_effectiveness_score'] * 0.2
                )
                comparison_df['overall_rank'] = comparison_df['overall_score'].rank(ascending=False)
        
        return comparison_df
    
    def _calculate_sensitivity_metrics(self, sensitivity_results: Dict[str, Any]) -> Dict[str, Any]:
        """ë¯¼ê°ë„ ì§€í‘œ ê³„ì‚°"""
        metrics = {}
        base_performance = 0
        
        # ê¸°ë³¸ ì„±ê³¼ ì ìˆ˜ ì¶”ì¶œ
        base_result = sensitivity_results['base_result']
        if 'analysis_results' in base_result and 'top_performers' in base_result['analysis_results']:
            performers_df = base_result['analysis_results']['top_performers']
            if not performers_df.empty:
                base_performance = performers_df['PERFORMANCE_SCORE'].mean()
        
        # ê° íŒŒë¼ë¯¸í„°ë³„ ë¯¼ê°ë„ ê³„ì‚°
        for param_name, param_results in sensitivity_results['parameter_variations'].items():
            performance_changes = []
            
            for result in param_results:
                if result['success'] and 'analysis_results' in result:
                    if 'top_performers' in result['analysis_results']:
                        performers_df = result['analysis_results']['top_performers']
                        if not performers_df.empty:
                            current_performance = performers_df['PERFORMANCE_SCORE'].mean()
                            change_pct = ((current_performance - base_performance) / base_performance * 100
                                        if base_performance > 0 else 0)
                            performance_changes.append({
                                'parameter_value': result['parameter_value'],
                                'performance_score': current_performance,
                                'change_percent': change_pct
                            })
            
            if performance_changes:
                # ë¯¼ê°ë„ ì§€í‘œ ê³„ì‚°
                changes = [pc['change_percent'] for pc in performance_changes]
                metrics[param_name] = {
                    'max_positive_change': max(changes) if changes else 0,
                    'max_negative_change': min(changes) if changes else 0,
                    'volatility': pd.Series(changes).std() if len(changes) > 1 else 0,
                    'performance_changes': performance_changes
                }
        
        return metrics
    
    def get_experiment_summary(self, scenario_name: str) -> str:
        """
        ì‹¤í—˜ ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            scenario_name: ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„
            
        Returns:
            str: ìš”ì•½ í…ìŠ¤íŠ¸
        """
        if scenario_name not in self.results:
            return f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario_name}' ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        result = self.results[scenario_name]
        
        if not result['success']:
            return f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario_name}' ì‹¤í–‰ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}"
        
        summary_lines = [
            f"=== ì‹¤í—˜ ìš”ì•½: {scenario_name} ===",
            f"ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ",
            f"ì‹¤í—˜ ë””ë ‰í† ë¦¬: {result['experiment_dir']}",
            ""
        ]
        
        # ìš”ì•½ í†µê³„
        summary_stats = result.get('summary_stats', {})
        if summary_stats:
            summary_lines.extend([
                "ğŸ“Š ì£¼ìš” í†µê³„:",
                f"â€¢ ì´ í• ë‹¹ ê±´ìˆ˜: {summary_stats.get('total_allocations', 0):,}",
                f"â€¢ ì´ í• ë‹¹ ìˆ˜ëŸ‰: {summary_stats.get('total_quantity', 0):,}",
                f"â€¢ ì»¤ë²„ëœ ë§¤ì¥ ìˆ˜: {summary_stats.get('stores_covered', 0)}",
                f"â€¢ í• ë‹¹ íš¨ìœ¨ì„±: {summary_stats.get('allocation_efficiency', 0):.1%}",
                ""
            ])
        
        # ë¶„ì„ ê²°ê³¼ ìš”ì•½
        analysis_results = result.get('analysis_results', {})
        
        if 'style_analysis' in analysis_results and not analysis_results['style_analysis'].empty:
            style_df = analysis_results['style_analysis']
            summary_lines.extend([
                "ğŸ¨ ìŠ¤íƒ€ì¼ ë¶„ì„:",
                f"â€¢ ë¶„ì„ëœ ìŠ¤íƒ€ì¼ ìˆ˜: {len(style_df)}",
                f"â€¢ í‰ê·  í™œìš©ë¥ : {style_df['UTILIZATION_RATE'].mean():.1%}",
                f"â€¢ í‰ê·  ìƒ‰ìƒ ì»¤ë²„ë¦¬ì§€: {style_df['COLOR_COVERAGE_RATE'].mean():.1%}",
                f"â€¢ í‰ê·  ì‚¬ì´ì¦ˆ ì»¤ë²„ë¦¬ì§€: {style_df['SIZE_COVERAGE_RATE'].mean():.1%}",
                ""
            ])
        
        if 'top_performers' in analysis_results and not analysis_results['top_performers'].empty:
            performers_df = analysis_results['top_performers']
            summary_lines.extend([
                "ğŸ† ë§¤ì¥ ì„±ê³¼:",
                f"â€¢ ë¶„ì„ëœ ìƒìœ„ ë§¤ì¥ ìˆ˜: {len(performers_df)}",
                f"â€¢ í‰ê·  ì„±ê³¼ ì ìˆ˜: {performers_df['PERFORMANCE_SCORE'].mean():.3f}",
                f"â€¢ ìµœê³  ì„±ê³¼ ì ìˆ˜: {performers_df['PERFORMANCE_SCORE'].max():.3f}",
                ""
            ])
        
        if 'scarce_effectiveness' in analysis_results and not analysis_results['scarce_effectiveness'].empty:
            scarce_df = analysis_results['scarce_effectiveness']
            summary_lines.extend([
                "ğŸ’ í¬ì†Œ SKU íš¨ê³¼ì„±:",
                f"â€¢ ë¶„ì„ëœ í¬ì†Œ SKU ìˆ˜: {len(scarce_df)}",
                f"â€¢ í‰ê·  íš¨ê³¼ì„± ì ìˆ˜: {scarce_df['EFFECTIVENESS_SCORE'].mean():.3f}",
                f"â€¢ ìµœê³  íš¨ê³¼ì„± ì ìˆ˜: {scarce_df['EFFECTIVENESS_SCORE'].max():.3f}",
                ""
            ])
        
        # ìƒì„±ëœ íŒŒì¼ ì •ë³´
        plot_files = result.get('plot_files', {})
        if plot_files:
            summary_lines.extend([
                "ğŸ“ˆ ìƒì„±ëœ ì‹œê°í™”:",
                *[f"â€¢ {plot_type}: {os.path.basename(filepath)}" 
                  for plot_type, filepath in plot_files.items()],
                ""
            ])
        
        return "\n".join(summary_lines)
    
    def cleanup_old_experiments(self, days_old: int = 7):
        """ì˜¤ë˜ëœ ì‹¤í—˜ ê²°ê³¼ ì •ë¦¬"""
        self.logger.info(f"{days_old}ì¼ ì´ìƒ ëœ ì‹¤í—˜ ê²°ê³¼ ì •ë¦¬ ì¤‘...")
        
        file_manager = FileManager()
        cleaned_count = file_manager.cleanup_old_experiments(days_old)
        
        self.logger.info(f"{cleaned_count}ê°œì˜ ì˜¤ë˜ëœ ì‹¤í—˜ í´ë”ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        return cleaned_count 